{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "import ramses2\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 15\n",
      "{'Ra': 1, 'Rc': 2, 'Rb01': 3, 'Rb02': 4, 'Rcu01': 5, 'Ru01': 6, 'Ru02': 7, 'Ru04': 8, 'Ru05': 9, 'Ru06': 10, 'X01': 11, 'Coin': 12, 'X02': 13, 'X03': 14, 'Rg': 15}\n"
     ]
    }
   ],
   "source": [
    "# min cls index must be 1 (0 is always bg, it is not needed in the dict)\n",
    "cls_to_idx = {\n",
    "    \"Ra\":1,\n",
    "    \"Rc\":2,\n",
    "    \"Rb01\":3,\n",
    "    \"Rb02\": 4,\n",
    "    \"Rcu01\":5,\n",
    "    \"Ru01\": 6,\n",
    "    \"Ru02\": 7,\n",
    "    \"Ru04\": 8,\n",
    "    \"Ru05\": 9,\n",
    "    \"Ru06\": 10,\n",
    "    \"X01\": 11,\n",
    "    \"Coin\":12,\n",
    "    \"X02\":13,\n",
    "    \"X03\":14,\n",
    "    \"Rg\":15\n",
    "    }\n",
    "\n",
    "idx_to_cls = {v: k for k, v in cls_to_idx.items()}\n",
    "ncls = len(cls_to_idx)\n",
    "print(f\"Number of classes: {len(cls_to_idx)}\")\n",
    "print(f\"{cls_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid sizes (H, W): [(96, 144), (48, 72), (24, 36), (12, 18)]\n",
      "Number of locations: 18360\n",
      "Mask shape: [256 384] stride 8\n",
      "Backbone strides [4, 8, 16, 32]\n",
      "Scale Ranges: [[1, 128], [64, 256], [128, 512], [256, 3072]]\n",
      "Grid sizes [144, 72, 36, 18]\n",
      "Approximate dmin and dmax in pixels for each level\n",
      "FPN Stride 8 (FPN level 4)           0.04,           4.50\n",
      "FPN Stride 16 (FPN level 5)           1.12,           4.50\n",
      "FPN Stride 32 (FPN level 6)           1.12,           4.50\n",
      "FPN Stride 64 (FPN level 7)           1.12,           13.50\n"
     ]
    }
   ],
   "source": [
    "target_shape = (4096 // 2 , 6144 // 2)  # image are resized to input_shape\n",
    "input_shape = target_shape + (3,)\n",
    "STEM_STRIDE = 4\n",
    "backbone_strides = [4, 8, 16, 32]  # backbone level C2 etc...\n",
    "FPN_input_strides = [8, 16, 32] # # C3 -> C5\n",
    "FPN_output_strides = [8, 16, 32, 64] # P3 -> P6\n",
    "# strides of the FPN levels used in the SOLO head\n",
    "fpn_fusion_level = 0 # Merge all FPN level to level FPN level 0\n",
    "first_grid_level = 0 # first FPN level used in the heads\n",
    "upscale_FPN_output = False  # if True, the FPN output is upsampled one time to produce the unified mask representation.\n",
    "mask_stride = FPN_output_strides[fpn_fusion_level]//2 if upscale_FPN_output else FPN_output_strides[fpn_fusion_level]\n",
    "mask_shape = np.array(target_shape) // mask_stride\n",
    "BACKBONE_LEVELS = 4  # number of levels in backbone\n",
    "offset_factor = 0.75  # reduce the size of the target box in SOLO class head (1 = full box, 0.5 = half box)\n",
    "\n",
    "# Grid sizes of the solo heads\n",
    "grid_sizes = [144, 72, 36, 18] #\n",
    "\n",
    "H, W = target_shape\n",
    "maxdim = max(target_shape)\n",
    "mindim = min(target_shape)\n",
    "grid_HW = [(int(round(gs * H / maxdim)), (int(round(gs * W/maxdim)))) for gs in grid_sizes]\n",
    "print(\"Grid sizes (H, W):\", grid_HW)\n",
    "nloc = sum([nx * ny for nx, ny in grid_HW])\n",
    "print(\"Number of locations:\", nloc)\n",
    "\n",
    "scale_ranges = [[1, 128]]   # for 1280 * 1920 and grid size 144\n",
    "\n",
    "factor = 0.5\n",
    "for i in range(len(FPN_output_strides) - 1):\n",
    "    scale_ranges.append([int(factor * scale_ranges[-1][1]), scale_ranges[-1][1] * 2])\n",
    "scale_ranges[-1][1] = int(np.max(target_shape))\n",
    "\n",
    "print(\"Mask shape:\", mask_shape, \"stride\", mask_stride)\n",
    "print(\"Backbone strides\", backbone_strides)\n",
    "print(\"Scale Ranges:\", scale_ranges)\n",
    "print(\"Grid sizes\", grid_sizes)\n",
    "print(\"Approximate dmin and dmax in pixels for each level\")\n",
    "for i, (dmin, dmax) in enumerate(scale_ranges):\n",
    "    level = 1 + np.log2(FPN_output_strides[i])\n",
    "    print(\n",
    "        f\"FPN Stride {FPN_output_strides[i]} (FPN level {int(level)}) \\\n",
    "          {offset_factor * dmin * grid_sizes[i] / (target_shape[1] ):.2f}, \\\n",
    "          {offset_factor * dmax * grid_sizes[i] / (target_shape[1] ):.2f}\"\n",
    "    )\n",
    "\n",
    "# Config for Convnext - pretrained model\n",
    "activation = \"GELU\"\n",
    "normalization = \"LayerNorm2d\"\n",
    "normalization_kw = {\"eps\":1e-6}\n",
    "bb = \"convnextv2_femto\"\n",
    "backbone_feature_nodes = {\n",
    "    \"stages.0\": \"C2\",\n",
    "    \"stages.1\": \"C3\",\n",
    "    \"stages.2\": \"C4\",\n",
    "    \"stages.3\": \"C5\",\n",
    "}\n",
    "backbone_params = {\"build_head\":True} # we need the head to load the full state_dict\n",
    "load_backbone = \"../checkpoints/backbones/convnextv2_femto_1k_224_ema.pt\"\n",
    "backbone_source=\"local\"\n",
    "# dims=[48, 96, 192, 384]\n",
    "connection_layers = {\"C3\": 96, \"C4\": 192, \"C5\": 384}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_backbone:../checkpoints/backbones/convnextv2_femto_1k_224_ema.pt\n",
      "backbone_params:{'build_head': True}\n",
      "backbone:convnextv2_femto\n",
      "backbone_source:local\n",
      "backbone_feature_nodes:{'stages.0': 'C2', 'stages.1': 'C3', 'stages.2': 'C4', 'stages.3': 'C5'}\n",
      "ncls:15\n",
      "imshape:(2048, 3072, 3)\n",
      "mask_stride:8\n",
      "activation:GELU\n",
      "normalization:LayerNorm2d\n",
      "normalization_kw:{'eps': 1e-06}\n",
      "connection_layers:{'C3': 96, 'C4': 192, 'C5': 384}\n",
      "FPN_filters:256\n",
      "extra_FPN_layers:1\n",
      "strides:[8, 16, 32, 64]\n",
      "head_layers:4\n",
      "head_filters:256\n",
      "kernel_size:1\n",
      "grid_sizes:[144, 72, 36, 18]\n",
      "point_nms:False\n",
      "mask_mid_filters:128\n",
      "mask_output_filters:256\n",
      "geom_feat_convs:4\n",
      "geom_feats_filters:128\n",
      "mask_output_level:0\n",
      "FPN_output_upscaling:False\n",
      "sigma_nms:0.5\n",
      "min_area:0\n",
      "use_binary_masks:True\n",
      "lossweights:[1.0, 1.0, 1.0]\n",
      "max_pos_samples:512\n",
      "scale_ranges:[[1, 128], [64, 256], [128, 512], [256, 3072]]\n",
      "offset_factor:0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sympy import true\n",
    "\n",
    "params = {\n",
    "    # backbone\n",
    "    \"load_backbone\": load_backbone,\n",
    "    \"backbone_source\": backbone_source,\n",
    "    \"backbone_feature_nodes\": backbone_feature_nodes,\n",
    "    \"backbone_params\": backbone_params,\n",
    "    \"backbone\": bb,\n",
    "    # Specific params\n",
    "    \"ncls\": ncls,\n",
    "    \"imshape\": input_shape,\n",
    "    \"mask_stride\": mask_stride,  # It must match with the backbone and the param 'output_level'\n",
    "    # General layers params\n",
    "    \"activation\": activation,\n",
    "    \"normalization\": normalization,\n",
    "    \"normalization_kw\": normalization_kw,\n",
    "    # FPN connection_layer smust be a dict with the id of the layer as key and its output channels as value\n",
    "    \"connection_layers\": connection_layers,  # backbone connection layers,\n",
    "    \"FPN_filters\": 256,\n",
    "    \"extra_FPN_layers\": 1,  # layers after P5. Strides must correspond to the number of FPN layers !\n",
    "    # SOLO head\n",
    "    \"strides\": FPN_output_strides,  # strides of FPN levels used in the heads [used to compute targets]\n",
    "    \"head_layers\": 4,  # Number of repeats of head conv layers\n",
    "    \"head_filters\": 256,\n",
    "    \"kernel_size\": 1,\n",
    "    \"grid_sizes\": grid_sizes,\n",
    "    # SOLO MASK head\n",
    "    \"point_nms\": False,\n",
    "    \"mask_mid_filters\": 128,\n",
    "    \"mask_output_filters\": 256,\n",
    "    \"geom_feat_convs\": 4,  # number of convs in the geometry factor branch\n",
    "    \"geom_feats_filters\": 128,\n",
    "    \"mask_output_level\": 0,  # size of the unified mask (in level of the FPN)\n",
    "    \"FPN_output_upscaling\": upscale_FPN_output, #if True, upscale the FPN fusion\n",
    "    # For inference\n",
    "    \"use_binary_masks\": True,\n",
    "    \"sigma_nms\": 0.5,\n",
    "    \"min_area\": 0,\n",
    "    # target allocation\n",
    "    \"scale_ranges\":scale_ranges,\n",
    "    \"offset_factor\":offset_factor\n",
    "}\n",
    "\n",
    "config = ramses2.Config(**params)\n",
    "\n",
    "# loss and training parameters\n",
    "lossweights = [1.0, 1.0, 1.0]\n",
    "max_pos_samples = 512  # limit the number of positive gt samples when computing loss to limit memory footprint\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create network from existing config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'build_head': True}\n"
     ]
    }
   ],
   "source": [
    "#Either using the previous config or loading an existing one\n",
    "config_path = Path(\"../checkpoints/2048x3072\")\n",
    "config_name = Path(\"config.json\")\n",
    "with open(config_path / config_name, 'r') as jsonfile:\n",
    "    params = json.load(jsonfile)\n",
    "config = ramses2.Config(**params)\n",
    "model = ramses2.RAMSESModel(config)\n",
    "# config.save(os.path.join(config_path, config_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = Path(\"../checkpoints/2048x3072/best-val-loss.pt\")\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Caution: if you want to load a *.ckpt from lightning use :\n",
    "# state_dict_cleaned = {}\n",
    "# state_dict_raw = torch.load(checkpoint_path)[\"state_dict\"]\n",
    "# for key, value in state_dict_raw.items():\n",
    "#     # Get the model\n",
    "#     if key.startswith(\"model.\"):\n",
    "#         new_key = key[6:]  # On retire le pr√©fixe 'model.'\n",
    "#         state_dict_cleaned[new_key] = value\n",
    "#     else:\n",
    "#         state_dict_cleaned[key] = value\n",
    "# model.load_state_dict(state_dict_cleaned, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load DatasetManager\n",
    "\n",
    "The DatasetManager is useful to generate training and valid sets. </br>\n",
    "It has methods to filter the images based on various criterion. </br>\n",
    "It contains annotations and train/valid filenames to build a torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset stats\n",
      "number of images in training set: 2897\n",
      "number of unique images in training set: 1750\n",
      "number of instances per class\n",
      "{'Coin': 549, 'Pl': 0, 'Ra': 13539, 'Rb01': 14270, 'Rb02': 6348, 'Rc': 19749, 'Rcu01': 1056, 'Rg': 271, 'Ru01': 19843, 'Ru02': 14246, 'Ru04': 10193, 'Ru05': 10876, 'Ru06': 4734, 'SHELLS': 0, 'UNKNOWN': 0, 'X01': 2506, 'X02': 1148, 'X03': 1492, 'X04': 0}\n",
      "Valid dataset stats\n",
      "number of images in valid set: 258\n",
      "number of unique images in valid set: 258\n",
      "number of instances per class\n",
      "{'Coin': 0, 'Pl': 0, 'Ra': 500, 'Rb01': 500, 'Rb02': 499, 'Rc': 500, 'Rcu01': 0, 'Rg': 0, 'Ru01': 500, 'Ru02': 500, 'Ru04': 500, 'Ru05': 500, 'Ru06': 499, 'SHELLS': 0, 'UNKNOWN': 0, 'X01': 502, 'X02': 0, 'X03': 0, 'X04': 0}\n"
     ]
    }
   ],
   "source": [
    "ds_path = Path(\"/PATH/TO/DATASET/FILES\")    # csv and json files generated using DatasetManager\n",
    "ds_name = \"15CLS_20250723-173206_MASS_ONLY\" # basename o fthe files\n",
    "# ds_name = \"15CLS_20250723-173206\"\n",
    "\n",
    "dataloader = ramses2.DatasetManager.from_file(\n",
    "    annfile=ds_path / Path(ds_name + \".csv\"), filename=ds_path / Path(ds_name + \".json\"),\n",
    ")\n",
    "if len(dataloader.train_basenames) > 0:\n",
    "    print(\"\\nTrain dataset stats\")\n",
    "    print(\"number of images in training set:\", len(dataloader.train_basenames))\n",
    "    print(\"number of unique images in training set:\", np.unique(dataloader.train_basenames).size)\n",
    "    print(\"number of instances per class\")\n",
    "    print(dataloader.train_class_counts)\n",
    "\n",
    "if len(dataloader.valid_basenames) > 0:\n",
    "    print(\"Valid dataset stats\")\n",
    "    print(\"number of images in valid set:\", len(dataloader.valid_basenames))\n",
    "    print(\"number of unique images in valid set:\", np.unique(dataloader.valid_basenames).size)\n",
    "    print(\"number of instances per class\")\n",
    "    print(dataloader.valid_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Torch Dataset\n",
    "TorchDataset creates a torch train/test Datasets using the DatasetManager annotations and filenames.</br>\n",
    "Some augmentations are implemented (brightness, noise, rotation) </br>\n",
    "CutMix is implemented in the collate function used in the Torch Dataloader </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 (2048, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(mask_stride, target_shape)\n",
    "train_dataset = ramses2.torchDataset(\n",
    "    dataloader.annotations,\n",
    "    dataloader.train_basenames,\n",
    "    input_shape=target_shape,\n",
    "    mask_stride=mask_stride,\n",
    "    cls_to_idx=cls_to_idx,\n",
    "    transform=ramses2.TorchAugmentations(probability=[0.33, 0.33, 0.33], factor=0.3, seed=0),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    random_resize_method=True,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "valid_dataset = ramses2.torchDataset(\n",
    "    dataloader.annotations,\n",
    "    dataloader.valid_basenames,\n",
    "    input_shape=target_shape,\n",
    "    mask_stride=mask_stride,\n",
    "    cls_to_idx=cls_to_idx,\n",
    "    transform=ramses2.TorchAugmentations(probability=[0.0, 0.0, 0.0], factor=0.0, seed=0),\n",
    "    crop_to_aspect_ratio=True,\n",
    "    random_resize_method=True,\n",
    "    seed=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9dede8a5bd467db373216005299dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Freeze backbone')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59746f1a430f4e2eb67e863b827684d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Freeze FPN')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f351ac8353924a308d80935a0b0ad198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Freeze cls head')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bdfb0724584171a97197b359f40039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Freeze kernel head & masks')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a866849c64e4e37b428a066907a6c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Freeze all density layers')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = widgets.Checkbox(value=False, description='Freeze backbone')\n",
    "w2 = widgets.Checkbox(value=False, description='Freeze FPN')\n",
    "w3 = widgets.Checkbox(value=False, description='Freeze cls head')\n",
    "w4 = widgets.Checkbox(value=False, description='Freeze kernel head & masks')\n",
    "w5 = widgets.Checkbox(value=False, description='Freeze all density layers')\n",
    "print(\"\")\n",
    "display(w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared_heads.class_factor_head.0.ops.1.weight      | FROZEN\n",
      "shared_heads.class_factor_head.0.ops.2.weight      | FROZEN\n",
      "shared_heads.class_factor_head.0.ops.2.bias        | FROZEN\n",
      "shared_heads.class_factor_head.1.ops.1.weight      | FROZEN\n",
      "shared_heads.class_factor_head.1.ops.2.weight      | FROZEN\n",
      "shared_heads.class_factor_head.1.ops.2.bias        | FROZEN\n",
      "shared_heads.class_factor_out.ops.1.weight         | FROZEN\n",
      "shared_heads.class_factor_out.ops.1.bias           | FROZEN\n",
      "mask_head.geom_convs.0.ops.1.weight                | FROZEN\n",
      "mask_head.geom_convs.0.ops.2.weight                | FROZEN\n",
      "mask_head.geom_convs.0.ops.2.bias                  | FROZEN\n",
      "mask_head.geom_convs.1.ops.1.weight                | FROZEN\n",
      "mask_head.geom_convs.1.ops.2.weight                | FROZEN\n",
      "mask_head.geom_convs.1.ops.2.bias                  | FROZEN\n",
      "mask_head.geom_convs.2.ops.1.weight                | FROZEN\n",
      "mask_head.geom_convs.2.ops.2.weight                | FROZEN\n",
      "mask_head.geom_convs.2.ops.2.bias                  | FROZEN\n",
      "mask_head.geom_convs.3.ops.1.weight                | FROZEN\n",
      "mask_head.geom_convs.3.ops.2.weight                | FROZEN\n",
      "mask_head.geom_convs.3.ops.2.bias                  | FROZEN\n",
      "mask_head.geom_final.ops.1.weight                  | FROZEN\n",
      "mask_head.geom_final.ops.1.bias                    | FROZEN\n",
      "\n",
      "Total parameters: 15,314,177\n",
      "Trainable parameters: 13,540,223\n",
      "Frozen parameters: 1,773,954\n"
     ]
    }
   ],
   "source": [
    "freeze_backbone = w1.value\n",
    "freeze_FPN = w2.value\n",
    "freeze_cls_head = w3.value\n",
    "freeze_kernel_mask_head = w4.value\n",
    "freeze_density_layers = w5.value\n",
    "\n",
    "for  param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "if freeze_backbone:\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if freeze_FPN:\n",
    "    for param in model.FPN.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if freeze_cls_head:\n",
    "    for param in model.shared_heads.class_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.shared_heads.class_logits.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if freeze_kernel_mask_head:\n",
    "    # kernel head\n",
    "    for param in model.shared_heads.kernel_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.shared_heads.kernel_out.parameters():\n",
    "        param.requires_grad = False\n",
    "    # mask head\n",
    "    for param in model.mask_head.level_pipelines.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.mask_head.mask_out.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if freeze_density_layers:\n",
    "    # class factor head\n",
    "    for param in model.shared_heads.class_factor_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.shared_heads.class_factor_out.parameters():\n",
    "        param.requires_grad = False\n",
    "    # geometry factor head\n",
    "    for param in model.mask_head.geom_convs.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.mask_head.geom_final.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        status = \"TRAINABLE\"\n",
    "    else:\n",
    "        status = \"FROZEN\"\n",
    "        print(f\"{name:<50} | {status}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "total_params = trainable_params + frozen_params\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING parameters\n",
    "Set Callbacks, learning rates, dataloader..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run will be saved in /media/jlux/SSD1/Projets/ARCADE/RAMSES2/runs/1024x1536/SOLO__newgrid_20251003-133303\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# lr plus petit pour la branche de segmentation\u001b[39;00m\n\u001b[1;32m     51\u001b[0m kernel_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_head\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel_out\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_head\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     54\u001b[0m         kernel_params\u001b[38;5;241m.\u001b[39madd(param)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "nx, ny = target_shape\n",
    "# set your own log directory !\n",
    "logdir = Path(f\"PATH/TO/YOUR_LOGDIR_{now}\")\n",
    "\n",
    "print(\"Run will be saved in\", logdir)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "loggers = [TensorBoardLogger(logdir / Path(\"tb_logs\"), name=\"RAMSES2\"), CSVLogger(logdir, name=\"RAMSES2\")]\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor=\"val_total_loss\", filename=os.path.join(logdir, \"best-val_loss\"), mode=\"min\", save_top_k=2, save_last=False\n",
    "))\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor=\"val_precision\", filename=os.path.join(logdir, \"best-val-prec_loss\"), mode=\"max\", save_top_k=1, save_last=False\n",
    "))\n",
    "\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor=\"train_total_loss\",\n",
    "    filename=os.path.join(logdir, \"best-train_loss\"),\n",
    "    mode=\"min\",\n",
    "    save_top_k=2,\n",
    "    save_last=False,\n",
    "))\n",
    "\n",
    "callbacks.append(LearningRateMonitor(logging_interval=\"epoch\"))\n",
    "# callbacks.append(EarlyStopping(monitor=\"val_density_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\"))\n",
    "callbacks.append(EarlyStopping(monitor=\"val_precision\", min_delta=0.00, patience=10, verbose=False, mode=\"max\"))\n",
    "callbacks.append(TQDMProgressBar(leave=True))\n",
    "\n",
    "train_config = ramses2.TrainConfig(\n",
    "    lr=1e-5,\n",
    "    mask_quality_weighting=True,\n",
    "    losses={\"cls\": True, \"seg\": True, \"mass\": False}, seg_loss=\"dice\", label_smoothing=0, cls_threshold=0.5\n",
    ")\n",
    "\n",
    "scheduler_config = [\n",
    "    {\n",
    "        \"scheduler\": ReduceLROnPlateau,\n",
    "        \"mode\": \"min\",\n",
    "        \"patience\": 3,\n",
    "        \"factor\": 0.5,\n",
    "        \"monitor\": \"train_total_loss\",\n",
    "        \"interval\": \"epoch\",\n",
    "        \"frequency\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set a small lr for kernel and mask head to avoid divergence\n",
    "kernel_params = set()\n",
    "for name, param in model.named_parameters():\n",
    "    if (\"kernel_head\" in name or \"kernel_out\" in name or name.startswith(\"mask_head\")) and param.requires_grad:\n",
    "        kernel_params.add(param)\n",
    "\n",
    "bb_params = set()\n",
    "for name, param in model.named_parameters():\n",
    "    if \"backbone\" in name and param.requires_grad:\n",
    "        bb_params.add(param)\n",
    "\n",
    "other_params = [p for p in model.parameters() if p.requires_grad and ((p not in kernel_params) and (p not in bb_params))]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": other_params, \"lr\": 1e-4, \"weight_decay\":5e-3},\n",
    "        {\"params\": list(kernel_params), \"lr\": 1e-5, \"weight_decay\":5e-3},\n",
    "        {\"params\": list(bb_params), \"lr\": 1e-5, \"weight_decay\":5e-3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=5e-4)\n",
    "\n",
    "model.config.max_pos_samples = 768\n",
    "\n",
    "lightning_model = ramses2.RAMSESLightning(\n",
    "    config, train_config=train_config, optimizer=optimizer, scheduler_config=scheduler_config\n",
    ")\n",
    "\n",
    "# Here we use a lightning model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=loggers,\n",
    "    num_sanity_val_steps=0,\n",
    "    gradient_clip_val=1,\n",
    ")\n",
    "batch_size = 8\n",
    "\n",
    "# Use collate_fn_cutmix only for instance segmentation training - not for mass prediction\n",
    "collate_fn = partial(ramses2.collate_fn_cutmix, p=0.5, mask_stride=config.mask_stride, max_patches=3, min_patch_ratio=0.1, max_patch_ratio=0.25)\n",
    "# just use collate_fn whent training mass prediction\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, collate_fn=collate_fn, pin_memory=True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8, collate_fn=ramses2.collate_fn, pin_memory=True\n",
    ")\n",
    "\n",
    "ntrain = len(train_dataset)\n",
    "nvalid = len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(logdir, exist_ok=True)\n",
    "print(\"save folder\", logdir)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "# Here we save the database in the config file saved in the logdir\n",
    "try:\n",
    "    config.database = os.path.join(ds_path, ds_name)\n",
    "    config.save(os.path.join(logdir, \"config.json\"))\n",
    "except AttributeError:\n",
    "    with open(Path(logdir) / Path(\"config.json\"), \"w\") as jsonfile:\n",
    "        json.dump(config, jsonfile)\n",
    "\n",
    "print(\"Losses:\", train_config.losses)\n",
    "\n",
    "trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "# trainer.fit(\n",
    "#     lightning_model,\n",
    "#     train_dataloaders=train_dataloader,\n",
    "#     val_dataloaders=valid_dataloader,\n",
    "#     ckpt_path=Path(\n",
    "#         \"/media/jlux/SSD1/Projets/ARCADE/RAMSES2/runs/logs/run_20250711-221033/best-train_loss-epoch=31.ckpt\"\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(valid_dataset)\n",
    "# model = model.to(torch.device('cuda:0'))\n",
    "model.eval()\n",
    "print(\"\")\n",
    "results_dataframe = pandas.DataFrame(columns=['Filename', 'PredictedMass', 'GTMass', 'RE'])\n",
    "# results_dataframe.set_index('Filename', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "for i in range(len(valid_dataset)):\n",
    "#inputs = next(iterator)\n",
    "    inputs = valid_dataset[i]\n",
    "    # print(inputs)\n",
    "    fn = inputs[\"filename\"]\n",
    "    gt_img = inputs[\"image\"]\n",
    "    gt_mask_img = inputs[\"masks\"]\n",
    "    gt_cls_ids = inputs[\"category_id\"]\n",
    "    gt_labels = inputs[\"label\"]\n",
    "    gt_mass = inputs[\"mass\"]\n",
    "    mask_res = inputs[\"res\"][0]\n",
    "    gt_cls_labels = [idx_to_cls[id] for id in list(gt_cls_ids.numpy())]\n",
    "    gt_img = inputs[\"image\"]\n",
    "    nx, ny = gt_img.shape[-2:]\n",
    "\n",
    "    print(\"Propcessing Image\", fn, gt_img.shape)\n",
    "    folder = dataloader.annotations.loc[dataloader.annotations[\"baseimg\"] == fn][\"folder\"].iloc[0]\n",
    "    print(\"folder\", folder)\n",
    "\n",
    "    res_ini = dataloader.annotations.loc[dataloader.annotations[\"baseimg\"] == fn][\"res\"].to_numpy()[0]\n",
    "    # The image may have been cropped, so we need to adjust the resolution\n",
    "    im = Image.open(os.path.join(folder, \"images\", fn + \".jpg\"))\n",
    "    width, height = im.size\n",
    "    res_input = res_ini * gt_img.shape[1] / height\n",
    "\n",
    "    print(f\"original resolution {res_ini} input resolution: {res_input}  mask resolution: {mask_res} ({res_input / config.mask_stride}) \")\n",
    "    model.config.sigma_nms = 0.5\n",
    "    print(\"number of gt instances\", gt_cls_ids.shape[-1])\n",
    "    # gt_img=gt_img.to(torch.device('cuda:0'))\n",
    "    with torch.no_grad():\n",
    "        results = model(gt_img.unsqueeze(0),\n",
    "                    training=False,\n",
    "                    cls_threshold=0.5,\n",
    "                    nms_threshold=0.3, # iou threshold or cls threshold in MatrixNMX\n",
    "                    mask_threshold=0.5,\n",
    "                    max_detections=768,\n",
    "                    scale_by_mask_scores=False,\n",
    "                    min_area=32,\n",
    "                    nms_mode=\"greedy\")[0]\n",
    "\n",
    "    # [{\"masks\": seg_preds, \"scores\": scores, \"cls_labels\": cls_labels_pos, \"masses\": masses}]\n",
    "\n",
    "    processed_masks = ramses2.decode_predictions(results['masks'], results[\"scores\"], threshold=0.5, by_mask_scores=False)\n",
    "    pred_masks = results['masks'].detach().cpu().numpy()\n",
    "    pred_masses = results['masses'].detach().cpu().numpy()\n",
    "    pred_cls_ids = results[\"cls_labels\"].detach().cpu().numpy()\n",
    "    pred_scores = results[\"scores\"].detach().cpu().numpy()\n",
    "\n",
    "    print(\"number of detected instances \", pred_masks.shape[0])\n",
    "    print(\"scores\", pred_scores)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    # gt_mass is not always defined\n",
    "    finite_indexes = np.where(np.isfinite(gt_mass.numpy()))\n",
    "    # unormalize mass predictions\n",
    "    gt_mass = gt_mass[finite_indexes] / (10*mask_res)**2\n",
    "    pred_masses = pred_masses / (10*mask_res)**2\n",
    "\n",
    "    print(\"GT total mass\", gt_mass.sum().item())\n",
    "    print(\"PRED total masses\", pred_masses.sum().item())\n",
    "    # print(\"GT masses\", np.sort(gt_mass))\n",
    "    # print(\"PRED masses\", np.sort(pred_masses))\n",
    "    pred_cls_labels = [idx_to_cls[id + 1] for id in list(pred_cls_ids)]\n",
    "    print(f\"GT class labels \\n{gt_cls_labels}\")\n",
    "    print(f\"PRED class labels \\n{pred_cls_labels}\")\n",
    "\n",
    "    new_row = [{'Filename': fn,\n",
    "            'PredictedMass': pred_masses.sum().item(),\n",
    "            'GTMass': gt_mass.sum().item(),\n",
    "            'RE': abs(pred_masses.sum().item() - gt_mass.sum().item()) / gt_mass.sum().item() if gt_mass.sum().item() > 0 else np.nan}]\n",
    "\n",
    "    results_dataframe = pandas.concat([results_dataframe, pandas.DataFrame(new_row)])\n",
    "\n",
    "\n",
    "    image = gt_img.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    fig = ramses2.plot_instances(\n",
    "        image,\n",
    "        processed_masks.cpu().detach().numpy(),\n",
    "        cls_ids=pred_cls_labels,\n",
    "        cls_scores=pred_scores,\n",
    "        alpha=0.4,\n",
    "        fontsize=3,\n",
    "        fontcolor=\"black\",\n",
    "        draw_boundaries=True,\n",
    "        boundary_mode=\"inner\",\n",
    "        dpi=200,\n",
    "        show=False,\n",
    "        x_offset=20,\n",
    "        y_offset=10,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(f\"/media/jlux/SSD1/Projets/ARCADE/RAMSES2/segmentations/SEG_{fn}.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe.to_excel(\"mass_res_per_img.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
